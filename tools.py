import os
from dotenv import load_dotenv

# LangGraph and Vertex AI imports
from langgraph.graph import StateGraph, END
from typing import TypedDict, List
import vertexai
from vertexai.generative_models import GenerativeModel

# Import the search client from our Phase 2 work
from google.cloud import discoveryengine_v1 as discoveryengine
from google.api_core.client_options import ClientOptions

# Load environment variables from .env file
load_dotenv()

# --- CONFIGURATION ---
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
GCP_LOCATION = os.getenv("GCP_LOCATION")
MODEL_NAME = os.getenv("FAST_MODEL")
DATA_STORE_ID = os.getenv("DATA_STORE_ID")

# --- 1. Define the State for the Graph ---

class GraphState(TypedDict):
    """Defines the state that is passed between nodes in the graph."""
    user_request: str           # The original request from the user
    research_results: str       # The context retrieved from the knowledge base
    final_proposal: str         # The final architectural proposal generated by the LLM

# --- 2. Define the Tool Functions (Nodes) ---

def survey_technologies_node(state: GraphState) -> GraphState:
    """This node performs research using our Vertex AI Search tool."""
    print("---NODE: Surveying Technologies---")
    user_request = state["user_request"]
    
    # This is the same search logic from our app.py
    client_options = ClientOptions(api_endpoint="discoveryengine.googleapis.com")
    client = discoveryengine.SearchServiceClient(client_options=client_options)
    serving_config = client.serving_config_path(
        project=GCP_PROJECT_ID,
        location="global",
        data_store=DATA_STORE_ID,
        serving_config="default_config",
    )
    request = discoveryengine.SearchRequest(serving_config=serving_config, query=user_request, page_size=10)
    response = client.search(request)

    results_str = ""
    for i, result in enumerate(response.results):
        doc = result.document
        content = doc.struct_data.get("content", "No content found.")
        source = doc.struct_data.get("source", "Unknown source.")
        results_str += f"Source: {source}\nContent: {content}\n\n"
    
    return {"research_results": results_str}

def synthesize_proposal_node(state: GraphState) -> GraphState:
    """This node takes the research and generates a final proposal using an LLM."""
    print("---NODE: Synthesizing Proposal---")
    prompt = f"""
    You are an expert Google Cloud solutions architect.
    A user wants to solve the following problem: "{state['user_request']}"

    Based on the following research from our internal knowledge base, please write a
    concrete, actionable architectural proposal. Explain your choices and reference
    the technologies found in the research.

    RESEARCH CONTEXT:
    {state['research_results']}
    """
    
    model = GenerativeModel(MODEL_NAME)
    response = model.generate_content(prompt)
    
    return {"final_proposal": response.text}

# --- 3. Define the Graph and Compile It ---

# Create a new graph
workflow = StateGraph(GraphState)

# Add the nodes to the graph
workflow.add_node("survey_technologies", survey_technologies_node)
workflow.add_node("synthesize_proposal", synthesize_proposal_node)

# Define the sequence of execution
workflow.set_entry_point("survey_technologies")
workflow.add_edge("survey_technologies", "synthesize_proposal")
workflow.add_edge("synthesize_proposal", END)

# Compile the graph into a runnable application
graph_app = workflow.compile()

# --- 4. Create a Wrapper Function for our Agent to Call ---

def propose_gcp_architecture(user_request: str) -> str:
    """The entry point for our advanced tool, callable by the main agent."""
    
    # Invoke the graph with the user's request
    final_state = graph_app.invoke({"user_request": user_request})
    
    # Return the final proposal
    return final_state["final_proposal"]